import chainlit as cl
import traceback
from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type
import anthropic
import httpx
from httpx import ReadError as HttpxReadError
import httpcore
from httpcore import ReadError as HttpcoreReadError
from langchain_anthropic import ChatAnthropic
from langchain.memory import ConversationBufferMemory
from langchain_core.messages import HumanMessage, SystemMessage

# ğŸ”§ ì „ì—­ ì„¸ì…˜ ë©”ëª¨ë¦¬ ìŠ¤í† ì–´
session_memory_store = {}
@retry(
    wait=wait_exponential(multiplier=1, min=2, max=10),
    stop=stop_after_attempt(3),
    retry=retry_if_exception_type((HttpxReadError, HttpcoreReadError, ConnectionError))
)
async def create_intelligent_summary_silent(messages_to_summarize, existing_summary=None):
    """APIë¥¼ í˜¸ì¶œí•˜ì—¬ ì˜ë¯¸ìˆëŠ” ìš”ì•½ ìƒì„±"""
    summary_model = ChatAnthropic(
        model="claude-3-haiku-20240307",
        temperature=0.1,
        streaming=False,
        timeout=60.0,
        max_retries=2
    )

    conversation_text = "\n\n".join(
        f"{'ì‚¬ìš©ì' if isinstance(msg, HumanMessage) else 'AI'}: {msg.content}"
        for msg in messages_to_summarize
    )

    if existing_summary:
        summary_prompt = f"""ë‹¤ìŒì€ ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ìš”ì•½ê³¼ ìƒˆë¡œ ì¶”ê°€ëœ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤. ê¸°ì¡´ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ëŒ€í™” ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©í•˜ì—¬ ì—…ë°ì´íŠ¸ëœ ì „ì²´ ìš”ì•½ë³¸ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

[ê¸°ì¡´ ìš”ì•½]
{existing_summary}

[ìƒˆë¡œìš´ ëŒ€í™” ë‚´ìš©]
{conversation_text}

[ì—…ë°ì´íŠ¸ëœ ì „ì²´ ìš”ì•½]:"""
    else:
        summary_prompt = f"""ë‹¤ìŒ ëŒ€í™” ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ í•µì‹¬ë§Œ ìš”ì•½í•´ì£¼ì„¸ìš”. ì´ ìš”ì•½ì€ ëŒ€í™”ì˜ ë§¥ë½ì„ ìœ ì§€í•˜ê¸° ìœ„í•œ ë‚´ë¶€ ì •ë³´ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.

[ëŒ€í™” ë‚´ìš©]
{conversation_text}

[ìš”ì•½]:"""

    try:
        summary_response = await summary_model.ainvoke([HumanMessage(content=summary_prompt)])
        summary_content = summary_response.content.strip()
        print(f"[DEBUG] ìš”ì•½ ìƒì„±/ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
        return summary_content
    except Exception as e:
        print(f"[DEBUG] ìš”ì•½ API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return existing_summary or f"ì´ì „ ëŒ€í™” {len(messages_to_summarize)}ê°œ (ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ)"

async def preprocess_with_silent_summary(input_data, load_system_prompt_func):
    """ëˆ„ì  ìš”ì•½ ê¸°ëŠ¥ì´ í¬í•¨ëœ ì „ì²˜ë¦¬ê¸°"""
    global session_memory_store
    session_id = cl.context.session.id

    if session_id not in session_memory_store:
        session_memory_store[session_id] = ConversationBufferMemory(
            return_messages=True, memory_key="chat_history"
        )
    memory = session_memory_store[session_id]

    user_message = input_data.get("messages", [])

    prompt_type = cl.user_session.get("prompt_type", "general")
    system_prompt = load_system_prompt_func(prompt_type)
    system_prompt_message = [SystemMessage(content=system_prompt)]

    try:
        memory_vars = memory.load_memory_variables({})
        chat_history = memory_vars.get("chat_history", [])

        existing_summary = None
        if chat_history and isinstance(chat_history[0], SystemMessage):
            if isinstance(chat_history[0].content, str) and chat_history[0].content.startswith("--- ëˆ„ì  ìš”ì•½ ---"):
                existing_summary = chat_history[0].content
                chat_history = chat_history[1:]
                print(f"[DEBUG] ê¸°ì¡´ ìš”ì•½ ë°œê²¬. ì‹¤ì œ ëŒ€í™” ê¸¸ì´: {len(chat_history)}")

        if len(chat_history) >= 4:
            messages_to_summarize = chat_history[:-2]
            recent_messages = chat_history[-2:]

            print(f"[DEBUG] ëˆ„ì  ìš”ì•½ ì‹œì‘. ê¸°ì¡´ ìš”ì•½:{'ìˆìŒ' if existing_summary else 'ì—†ìŒ'}, ìš”ì•½ ëŒ€ìƒ:{len(messages_to_summarize)}, ìœ ì§€:{len(recent_messages)}")

            updated_summary_content = await create_intelligent_summary_silent(messages_to_summarize, existing_summary)

            summary_message = SystemMessage(content=f"--- ëˆ„ì  ìš”ì•½ ---\n{updated_summary_content}")
            memory.chat_memory.messages = [summary_message] + recent_messages

            print(f"[DEBUG] ë©”ëª¨ë¦¬ ì¬êµ¬ì„± ì™„ë£Œ. í˜„ì¬ ë©”ëª¨ë¦¬: ìš”ì•½ 1ê°œ + ìµœì‹  ëŒ€í™” {len(recent_messages)}ê°œ")

            messages_for_llm = [summary_message] + recent_messages
        else:
            messages_for_llm = ([SystemMessage(content=existing_summary)] if existing_summary else []) + chat_history

        final_messages_to_agent = system_prompt_message + messages_for_llm + user_message

        print(f"[DEBUG] ìµœì¢… ì „ë‹¬ ë©”ì‹œì§€ ìˆ˜: {len(final_messages_to_agent)}")
        return {"messages": final_messages_to_agent}

    except Exception as e:
        print(f"[DEBUG] ì „ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
        return {"messages": system_prompt_message + user_message}

class AsyncConversationBufferMemory:
    """ConversationBufferMemoryë¥¼ RunnableWithMessageHistoryì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ë˜í¼"""
    def __init__(self, memory):
        self.memory = memory

    async def aget_messages(self):
        variables = self.memory.load_memory_variables({})
        chat_history = variables.get("chat_history", [])
        return chat_history if isinstance(chat_history, list) else []

    async def aadd_messages(self, messages, *args, **kwargs):
        print(f"[DEBUG][aadd_messages] called with {len(messages)} messages")
        for msg in messages:
            if hasattr(msg, 'type'):
                if msg.type == "human":
                    self.memory.save_context({"input": msg.content}, {"output": ""})
                elif msg.type == "ai":
                    self.memory.save_context({"input": ""}, {"output": msg.content})

    def __getattr__(self, name):
        return getattr(self.memory, name) 