# app_chainlit.py (ë¦¬íŒ©í† ë§ ì™„ë£Œ ë²„ì „)

# --- 1. ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
import chainlit as cl
import chainlit.types as cl_types
import chainlit.server as cl_server
import json
import pandas as pd
import io
from datetime import datetime
import traceback
import time
import asyncio
import re
import codecs
from typing import Any, List, Optional

# --- 2. MCP ë° Langchain ê´€ë ¨ íƒ€ì… ì„í¬íŠ¸ ---
from mcp import ClientSession
from mcp.types import CallToolResult, TextContent
from langchain_core.messages import HumanMessage, ToolMessage
from langchain.memory import ConversationBufferMemory
from langchain_core.runnables import RunnableConfig
from google.api_core.exceptions import ServiceUnavailable
import anthropic
# ë„¤íŠ¸ì›Œí¬ ê´€ë ¨
import httpx
from httpx import ReadError as HttpxReadError
import httpcore
from httpcore import ReadError as HttpcoreReadError
from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type

# --- 3. ë¶„ë¦¬ëœ ëª¨ë“ˆ ì„í¬íŠ¸ ---
from dotenv import load_dotenv
import utils.db_utils as db_utils
import utils.ui_utils as ui_utils
import utils.llm_setup as llm_setup
import utils.memory_manager as memory_manager

# --- 4. ì´ˆê¸° ì„¤ì • ---
load_dotenv(override=True)
db_utils.init_database()


# --- 5. ë¡œê·¸ì¸ ë° ì¸ì¦ ---
@cl.password_auth_callback
def auth_callback(username: str, password: str):
    """ì‚¬ìš©ì ì¸ì¦ ì½œë°±"""
    print(f"DEBUG: ë¡œê·¸ì¸ ì‹œë„ - ì‚¬ìš©ìëª…: {username}")
    user_data = db_utils.authenticate_user(username, password)
    print(f"DEBUG: ì¸ì¦ ê²°ê³¼: {user_data}")

    if user_data:
        print(f"DEBUG: ë¡œê·¸ì¸ ì„±ê³µ - {user_data['display_name']}")
        user = cl.User(
            identifier=user_data["username"],
            metadata={
                "user_id": user_data["id"],
                "username": user_data["username"],
                "display_name": user_data["display_name"],
                "db_id": user_data["id"]
            }
        )
        try:
            user.display_name = user_data["display_name"]
            print(f"DEBUG: display_name ì§ì ‘ ì„¤ì • ì™„ë£Œ: {user_data['display_name']}")
        except Exception as e:
            print(f"DEBUG: display_name ì„¤ì • ì‹¤íŒ¨: {e}")
        return user
    else:
        print(f"DEBUG: ë¡œê·¸ì¸ ì‹¤íŒ¨ - ì‚¬ìš©ìëª… ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ ë¶ˆì¼ì¹˜")
        return None

# --- 6. MCP ì—°ê²° í•¸ë“¤ëŸ¬ ---
@cl.on_mcp_connect
async def on_mcp_connect(connection, session: ClientSession):
    """MCP ì—°ê²° ì‹œ ë„êµ¬ ë° ì—ì´ì „íŠ¸ ì„¤ì •"""
    try:
        tool_metadatas = await session.list_tools()
        mcp_tools = cl.user_session.get("mcp_tools", {})
        mcp_tools[connection.name] = [
            {"name": t.name, "description": t.description, "input_schema": t.inputSchema}
            for t in tool_metadatas.tools
        ]
        cl.user_session.set("mcp_tools", mcp_tools)

        model_name = cl.user_session.get("model_name", "claude-sonnet-4-20250514")

        from langchain_core.tools import Tool
        all_langchain_tools = []
        for conn_name, tools in mcp_tools.items():
            for tool_info in tools:
                async def tool_func(tool_input: Any, conn_name=conn_name, tool_name=tool_info['name']):
                    mcp_session, _ = cl.context.session.mcp_sessions.get(conn_name)
                    if not mcp_session: return f"Error: MCP session for '{conn_name}' not found."
                    if not isinstance(tool_input, dict): tool_input = {"query": tool_input}
                    try:
                        return str(await mcp_session.call_tool(tool_name, tool_input))
                    except Exception as e: return f"Error calling tool {tool_name}: {e}"

                all_langchain_tools.append(
                    Tool(
                        name=tool_info['name'],
                        description=tool_info['description'],
                        func=tool_func,
                        coroutine=tool_func
                    )
                )

        # ë©”ëª¨ë¦¬ ê°ì²´ ìƒì„± ë° ì €ì¥
        session_id = cl.context.session.id
        if session_id not in memory_manager.session_memory_store:
            memory_manager.session_memory_store[session_id] = ConversationBufferMemory(
                return_messages=True, memory_key="chat_history"
            )
        
        # Agent core ìƒì„± -> Plan-and-Execute Graph ìƒì„±ìœ¼ë¡œ ë³€ê²½
        prompt_type = cl.user_session.get("prompt_type", "general") # í”„ë¡¬í”„íŠ¸ íƒ€ì… ê°€ì ¸ì˜¤ê¸°
        graph = llm_setup.create_plan_and_execute_graph(model_name, all_langchain_tools, prompt_type)
        cl.user_session.set("agent", graph)

        print("[DEBUG] MCP ì—°ê²° ì™„ë£Œ - Plan-and-Execute Graph ì„¤ì •ë¨")

    except Exception as e:
        error_msg = f"MCP ì—°ê²° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
        print(error_msg)
        traceback.print_exc()
        await ui_utils.send_error_with_reset_guidance("ë„êµ¬ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", "ì¼ë°˜")

@cl.on_mcp_disconnect
async def on_mcp_disconnect(connection):
    await cl.Message(content=f"MCP ì—°ê²° '{connection}'ì´(ê°€) ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.").send()

# --- 7. Chat Profile ë° ì‹œì‘ ì„¤ì • ---
def load_mcp_servers_from_config():
    """config.jsonì—ì„œ MCP ì„œë²„ ì„¤ì • ì½ê¸°"""
    try:
        with open("config.json", "r", encoding="utf-8") as f:
            config = json.load(f)
        return [{"name": name, "command": f"{conf['command']} {' '.join(conf.get('args', []))}".strip()}
                for name, conf in config.items()]
    except Exception as e:
        print(f"[DEBUG] config.json ì½ê¸° ì‹¤íŒ¨: {e}")
        return []

@cl.set_chat_profiles
async def chat_profiles():
    """ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ í”„ë¡¬í”„íŠ¸ ì¡°í•©ì˜ ì±— í”„ë¡œí•„ ì„¤ì •"""
    return [
        cl.ChatProfile(name="Claude 4 + General", markdown_description="Claude Sonnet 4 + ì¼ë°˜ í”„ë¡¬í”„íŠ¸"),
        cl.ChatProfile(name="Claude 4 + GoT deep", markdown_description="Claude Sonnet 4 + GoT ë°©ë²•ë¡  í”„ë¡¬í”„íŠ¸ (Deep)"),
        cl.ChatProfile(name="Claude 3.7 + GoT deep", markdown_description="Claude 3.7 Sonnet + GoT ë°©ë²•ë¡  í”„ë¡¬í”„íŠ¸ (Deep)"),
        cl.ChatProfile(name="Claude 3.7 + General", markdown_description="Claude 3.7 Sonnet + ì¼ë°˜ í”„ë¡¬í”„íŠ¸"),
        cl.ChatProfile(name="Gemini 2.5 Pro + GoT deep", markdown_description="Gemini 2.5 Pro + GoT ë°©ë²•ë¡  í”„ë¡¬í”„íŠ¸ (Deep)"),
        cl.ChatProfile(name="Gemini 2.5 Pro + General", markdown_description="Gemini 2.5 Pro + ì¼ë°˜ í”„ë¡¬í”„íŠ¸"),
        cl.ChatProfile(name="Gemini 2.5 Flash + GoT deep", markdown_description="Gemini 2.5 Flash + GoT ë°©ë²•ë¡  í”„ë¡¬í”„íŠ¸ (Deep)"),
        cl.ChatProfile(name="Gemini 2.5 Flash + General", markdown_description="Gemini 2.5 Flash + ì¼ë°˜ í”„ë¡¬í”„íŠ¸"),
    ]

@cl.on_chat_start
async def on_chat_start():
    """ì±„íŒ… ì‹œì‘ ì‹œ ëª¨ë¸, í”„ë¡¬í”„íŠ¸, MCP ì—°ê²° ì„¤ì •"""
    profile_map = {
        "Claude 4 + General": ("claude-sonnet-4-20250514", "general"),
        "Claude 4 + GoT deep": ("claude-sonnet-4-20250514", "got_deep"),
        "Claude 3.7 + GoT deep": ("claude-3-7-sonnet-latest", "got_deep"),
        "Claude 3.7 + General": ("claude-3-7-sonnet-latest", "general"),
        "Gemini 2.5 Flash + GoT deep": ("gemini-2.5-flash", "got_deep"),
        "Gemini 2.5 Flash + General": ("gemini-2.5-flash", "general"),
        "Gemini 2.5 Pro + GoT deep": ("gemini-2.5-pro", "got_deep"),
        "Gemini 2.5 Pro + General": ("gemini-2.5-pro", "general"),
    }
    chat_profile = cl.user_session.get("chat_profile", "Claude 4 + General")
    model_name, prompt_type = profile_map.get(chat_profile, profile_map["Claude 4 + General"])

    cl.user_session.set("model_name", model_name)
    cl.user_session.set("prompt_type", prompt_type)

    mcp_servers = load_mcp_servers_from_config()
    try:
        print("[DEBUG] ì§ì ‘ MCP ì—°ê²° ì‹œë„...")
        for i, server in enumerate(mcp_servers, 1):
            try:
                conn_request = cl_types.ConnectStdioMCPRequest(
                    sessionId=cl.context.session.id,
                    clientType="stdio", name=server["name"], fullCommand=server["command"]
                )
                await cl_server.connect_mcp(conn_request, cl.context.session.user)
                print(f"[DEBUG] MCP ì„œë²„ {i} ({server['name']}) ì—°ê²° ì„±ê³µ")
                if i < len(mcp_servers): await asyncio.sleep(0.5)
            except Exception as e:
                print(f"[DEBUG] MCP ì„œë²„ {i} ({server['name']}) ì—°ê²° ì‹¤íŒ¨: {e}")
    except Exception as e:
        print(f"[DEBUG] MCP ì—°ê²° ì „ì²´ ì‹¤íŒ¨: {e}")
        try:
            await cl.mcp.autoconnect(); print("[DEBUG] Fallback autoconnect ì„±ê³µ")
        except Exception as fallback_error:
            print(f"[DEBUG] Fallback autoconnectë„ ì‹¤íŒ¨: {fallback_error}")

    user = cl.user_session.get("user") or cl.context.session.user
    if not user:
        await cl.Message(content="ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤. ë‹¤ì‹œ ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”.").send()
        return
    
    cl.user_session.set("mcp_tools", {})
    cl.user_session.set("message_count", 0)
    display_name = user.metadata.get("display_name", "ì‚¬ìš©ì")
    await cl.Message(content=f"ì•ˆë…•í•˜ì„¸ìš” {display_name}ë‹˜! ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš” ğŸ˜Š").send()

@cl.on_chat_end
async def on_chat_end():
    """ì±„íŒ… ì¢…ë£Œ ì‹œ ì„¸ì…˜ ì €ì¥"""
    user = cl.user_session.get("user") or cl.context.session.user
    if not user: return

    if cl.user_session.get("message_count", 0) > 0:
        first_message = cl.user_session.get("first_message", "ìƒˆë¡œìš´ ëŒ€í™”")
        title = db_utils.generate_session_title(first_message)
        actual_user_id = user.metadata.get("user_id", user.identifier)
        db_utils.save_chat_session(cl.context.session.id, actual_user_id, title)

# --- 8. ë©”ì‹œì§€ ì²˜ë¦¬ í•¸ë“¤ëŸ¬ ---
@retry(
    wait=wait_exponential(multiplier=1, min=2, max=10), stop=stop_after_attempt(5),
    retry=retry_if_exception_type((HttpxReadError, HttpcoreReadError, ConnectionError, anthropic.APIStatusError, anthropic.APIConnectionError, anthropic.APITimeoutError, TimeoutError))
)
async def astream_events_with_retry(agent, graph_input, config):
    """ê°•í™”ëœ ì¬ì‹œë„ ë¡œì§ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¼ ì´ë²¤íŠ¸ ì²˜ë¦¬ (ì…ë ¥ í˜•ì‹ ì¼ë°˜í™”)"""
    async for event in agent.astream_events(graph_input, config=config, version="v2"):
        yield event

@cl.on_message
async def on_message(message: cl.Message):
    """ì‚¬ìš©ì ë©”ì‹œì§€ ì²˜ë¦¬"""
    user = cl.user_session.get("user") or cl.context.session.user
    if not user:
        await ui_utils.send_error_with_reset_guidance("ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.", "ì¼ë°˜")
        return

    # ë©”ì‹œì§€ ì¹´ìš´íŠ¸ ë° DB ì €ì¥
    msg_count = cl.user_session.get("message_count", 0) + 1
    cl.user_session.set("message_count", msg_count)
    if msg_count == 1: cl.user_session.set("first_message", message.content)
    
    session_id = cl.context.session.id
    try:
        db_utils.save_message(session_id, "user", message.content)
    except Exception as e:
        print(f"[DEBUG] DB ì €ì¥ ì‹¤íŒ¨: {e}")

    agent = cl.user_session.get("agent")
    if not agent:
        await ui_utils.send_error_with_reset_guidance("ì—ì´ì „íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "ì¼ë°˜")
        return

    # 1. Plan-and-Execute ê·¸ë˜í”„ ì…ë ¥ ì¤€ë¹„ (ëŒ€í™” ê¸°ë¡ ì¶”ê°€)
    memory = memory_manager.session_memory_store.get(session_id)
    chat_history = []
    if memory:
        # ConversationBufferMemoryì—ì„œ BaseMessage ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ê°€ì ¸ì˜µë‹ˆë‹¤.
        chat_history = memory.load_memory_variables({}).get("chat_history", [])

    # chat_historyë¥¼ PlanExecute ìƒíƒœì˜ ì¼ë¶€ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
    graph_input = {
        "input": message.content,
        "chat_history": chat_history,
        "summary": cl.user_session.get("summary", "") # ì„¸ì…˜ì—ì„œ ìš”ì•½ë³¸ ë¡œë“œ
    }
    
    # 2. ê·¸ë˜í”„ ì‹¤í–‰ ë° UI ë Œë”ë§
    config = RunnableConfig(recursion_limit=100, thread_id=session_id, callbacks=[cl.LangchainCallbackHandler()])
    
    final_response = "" # ì „ì²´ ì‘ë‹µì„ ì €ì¥í•  ë³€ìˆ˜ (ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•´ ""ë¡œ ì´ˆê¸°í™”)
    plan_msg = None
    agent_step = None # í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ì—ì´ì „íŠ¸ ìŠ¤í…ì„ ì¶”ì 
    response_msg = cl.Message(content="", author="Assistant") # ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ë¹ˆ ë©”ì‹œì§€ ê°ì²´
    
    try:
        async for event in astream_events_with_retry(agent, graph_input, config):
                kind = event["event"]
                name = event["name"]

                if kind == "on_chain_start":
                    if name == "planner":
                        pass
                        #await cl.Message(content="ğŸ¤” **ê³„íš ìˆ˜ë¦½ ì¤‘...**", author="System").send()
                    elif name == "agent":
                        plan = event["data"].get("input", {}).get("plan", [])
                        if plan:
                            task = plan[0]
                            agent_step = cl.Step(name=f"ğŸš€ ì‹¤í–‰: {task}", type="run")
                            await agent_step.send()
                    elif name == "replan":
                        if agent_step:
                            await agent_step.remove() # ì´ì „ ì—ì´ì „íŠ¸ ìŠ¤í… UI ì •ë¦¬
                            agent_step = None
                        #await cl.Message(content="ğŸ”„ **ê³„íš ê²€í†  ë° ë‹¤ìŒ ë‹¨ê³„ ì¤€ë¹„ ì¤‘...**", author="System").send()

                elif kind == "on_chain_stream":
                    # ìµœì¢… ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
                    chunk = event["data"].get("chunk")
                    response_chunk = None
                    if isinstance(chunk, str):
                        response_chunk = chunk
                    elif isinstance(chunk, dict) and "response" in chunk:
                        # LangChainì˜ Plan-and-Execute ê·¸ë˜í”„ì—ì„œ ìµœì¢… ì‘ë‹µì´ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì˜¬ ìˆ˜ ìˆìŒ
                        response_chunk = chunk["response"]

                    if response_chunk:
                        # ì—ì´ì „íŠ¸ì˜ ì²­í¬ê°€ ëˆ„ì ëœ ì „ì²´ ì‘ë‹µì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
                        # ì´ë¯¸ ìŠ¤íŠ¸ë¦¬ë°ëœ ë¶€ë¶„(final_response)ê³¼ ë¹„êµí•˜ì—¬ ìƒˆë¡œìš´ ë¶€ë¶„(delta)ë§Œ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.
                        if response_chunk.startswith(final_response):
                            delta = response_chunk[len(final_response):]
                        else:
                            delta = response_chunk # ì˜ˆìƒì¹˜ ëª»í•œ ê²½ìš°, ê·¸ëƒ¥ ì²­í¬ ì „ì²´ë¥¼ ë³´ëƒ…ë‹ˆë‹¤.
                        
                        if delta:
                            final_response += delta
                            await response_msg.stream_token(delta)

                elif kind == "on_chain_end":
                    if name == "planner":
                        plan = event["data"].get("output", {}).get("plan", [])
                        if plan: # ë¹ˆ ê³„íšì€ í‘œì‹œí•˜ì§€ ì•ŠìŒ
                            plan_text = "\n".join(f"- {step}" for step in plan)
                            plan_msg = cl.Message(content=f"**ğŸ“ ê³„íš:**\n{plan_text}", author="System")
                            await plan_msg.send()
                    elif name == "agent":
                         if agent_step:
                            result = event["data"].get("output", {}).get("past_steps", [("", "")])[-1][1]
                            agent_step.output = result
                            await agent_step.update()
                    elif name == "replan" or name == "__end__" or name == "agent":
                        output = event["data"].get("output", {})
                        
                        # ìŠ¤íŠ¸ë¦¬ë°ì´ ì´ë¯¸ ì§„í–‰ë˜ì—ˆë‹¤ë©´, ì—¬ê¸°ì„œëŠ” íŠ¹ë³„í•œ ì²˜ë¦¬ë¥¼ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
                        if final_response:
                            continue
                        
                        # 'replan', '__end__', 'agent' ë…¸ë“œ ëª¨ë‘ì—ì„œ ìµœì¢… ì‘ë‹µì´ 'response' í‚¤ë¡œ ì „ë‹¬ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                        if "response" in output:
                            final_response = output["response"]

                        # 'replan'ì˜ ê²½ìš°, ë‹¤ìŒ ê³„íšì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
                        elif "plan" in output and plan_msg:
                            new_plan = output["plan"]
                            if new_plan:
                                plan_text = "\n".join(f"- {step}" for step in new_plan)
                                plan_msg.content = f"ğŸ“ **ë‹¤ìŒ ê³„íš:**\n{plan_text}"
                                await plan_msg.update()
                            else:
                                # ìƒˆë¡œìš´ ê³„íšì´ ë¹„ì–´ìˆìœ¼ë©´, ëª¨ë“  ê³„íšì´ ì™„ë£Œë˜ì—ˆìŒì„ ì˜ë¯¸í•˜ë¯€ë¡œ UIë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
                                await plan_msg.remove()
        
        # ìŠ¤íŠ¸ë¦¬ë°ì´ ì™„ë£Œë˜ì—ˆê±°ë‚˜, ìŠ¤íŠ¸ë¦¬ë° ì—†ì´ ìµœì¢… ì‘ë‹µë§Œ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
        if response_msg.content: 
            await response_msg.update()
        elif final_response: 
             await cl.Message(content=final_response, author="Assistant").send()


    except (HttpxReadError, HttpcoreReadError, ConnectionError) as e:
        await ui_utils.send_error_with_reset_guidance("ë„¤íŠ¸ì›Œí¬ê°€ ë¶ˆì•ˆì •í•˜ì—¬ ì‘ë‹µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "ë„¤íŠ¸ì›Œí¬"); return
    except (anthropic.APIStatusError, ServiceUnavailable) as e:
        msg, err_type = ("ì„œë²„ ê³¼ë¶€í•˜ ìƒíƒœì…ë‹ˆë‹¤.", "API")
        if isinstance(e, ServiceUnavailable) and not ("overloaded" in str(e) or "503" in str(e)):
            msg, err_type = (f"API ì˜¤ë¥˜: {e}", "API")
        await ui_utils.send_error_with_reset_guidance(msg, err_type); return
    except (MemoryError, RecursionError) as e:
        err_type = "ë©”ëª¨ë¦¬" if isinstance(e, MemoryError) else "ë³µì¡ë„"
        await ui_utils.send_error_with_reset_guidance(f"{err_type} ë¬¸ì œë¡œ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "ë©”ëª¨ë¦¬"); return
    except Exception as e:
        print(f"[DEBUG] ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}\n{traceback.format_exc()}")
        await ui_utils.send_critical_error_guidance(); return
    if agent_step: await agent_step.remove()
    # 3. ìµœì¢… ì‘ë‹µ ë° ë©”ëª¨ë¦¬ ì €ì¥
    if final_response:
        # await cl.Message(content=final_response, author="Assistant").send() # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ëŒ€ì²´ë˜ì—ˆìœ¼ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬
        try:
            db_utils.save_message(session_id, "assistant", final_response)
            if session_id in memory_manager.session_memory_store:
                memory = memory_manager.session_memory_store[session_id]
                memory.save_context({"input": message.content.strip()}, {"output": final_response})
                print(f"[DEBUG] ëŒ€í™” ì €ì¥ ì™„ë£Œ. ë©”ëª¨ë¦¬ í¬ê¸°: {len(memory.load_memory_variables({})['chat_history'])}ê°œ")
        except Exception as e:
            print(f"[DEBUG] ìµœì¢… ì‘ë‹µ ì €ì¥ ì‹¤íŒ¨: {e}")
    elif not response_msg.content: # ìŠ¤íŠ¸ë¦¬ë°ë„, ìµœì¢…ì‘ë‹µë„ ì—†ëŠ” ê²½ìš°
        await ui_utils.send_error_with_reset_guidance("ì‘ë‹µ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", "ì¼ë°˜"); return
    
    # ìš”ì•½ ì—…ë°ì´íŠ¸ ë¡œì§ ì¶”ê°€
    if final_response:
        # 1. í˜„ì¬ ì„¸ì…˜ì˜ ì „ì²´ ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
        memory = memory_manager.session_memory_store.get(session_id)
        chat_history_for_summary = []
        if memory:
            chat_history_for_summary = memory.load_memory_variables({}).get("chat_history", [])

        # 2. ìš”ì•½í•  ë©”ì‹œì§€ê°€ ì¶©ë¶„íˆ ìˆì„ ê²½ìš°ì—ë§Œ ìš”ì•½ ì‹¤í–‰
        if len(chat_history_for_summary) > 2:
            try:
                new_summary = await llm_setup.create_intelligent_summary_silent(
                    messages_to_summarize=chat_history_for_summary,
                    existing_summary=cl.user_session.get("summary", "")
                )
                cl.user_session.set("summary", new_summary)
                print(f"[DEBUG] ëŒ€í™” ìš”ì•½ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
            except Exception as e:
                print(f"[DEBUG] ëŒ€í™” ìš”ì•½ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
